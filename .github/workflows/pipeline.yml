name: GitHub Crawler Pipeline

on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 06:00 UTC
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:

jobs:
  build-and-crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 300

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres_password
          POSTGRES_DB: github_crawler
        options:
          --health-cmd pg_isready
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
        ports:
          - 5432:5432

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: 'latest'
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: setup-postgres
        env:
          PGPASSWORD: postgres_password
        run: |
          psql -h localhost -U postgres -d github_crawler -f setup.sql

      - name: crawl-stars
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DATABASE_URL: postgresql+asyncpg://postgres:postgres_password@localhost:5432/github_crawler
        run: |
          uv run python -m src.main

      - name: Dump database contents
        env:
          PGPASSWORD: postgres_password
        run: |
          psql -h localhost -U postgres -d github_crawler -c "\COPY (SELECT * FROM github_repositories) TO 'repositories_dump.csv' WITH CSV HEADER;"

      - name: Upload Crawler Artifact
        uses: actions/upload-artifact@v4
        with:
          name: github-stars-data
          path: repositories_dump.csv
          retention-days: 7
           
